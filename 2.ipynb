{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32af3cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34d9fa13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "torch.set_printoptions(edgeitems=2, linewidth=75)\n",
    "\n",
    "t_c = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0]\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
    "t_c = torch.tensor(t_c)\n",
    "t_u = torch.tensor(t_u)\n",
    "\n",
    "plt.scatter(t_c, t_u)\n",
    "\n",
    "def model(t_u, w, b):\n",
    "    return w * t_u + b\n",
    "\n",
    "def loss_fn(t_p, t_c):\n",
    "    squared_diffs=(t_p - t_c)**2\n",
    "    return squared_diffs.mean()\n",
    "\n",
    "w = torch.ones(())\n",
    "b = torch.zeros(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db678eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "params=torch.tensor([1.0,0.0], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "034eeacb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5577e2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_p=model(t_u, *params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f769ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([35.7000, 55.9000, 58.2000, 81.9000, 56.3000, 48.9000, 33.9000,\n",
       "        21.8000, 48.4000, 60.4000, 68.4000], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "194f3ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=loss_fn(t_p, t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86f75ac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1763.8848, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5a569e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17681c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1763.8848, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "067b32d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4517.2969,   82.6000])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "740583e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d47b1f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "008123bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a4ac814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(epochs, le, params, t_u, t_c):\n",
    "    for epoch in range(epochs):\n",
    "        if params.grad != None:\n",
    "            params.grad.zero_()\n",
    "        t_p=model(t_u, *params)\n",
    "        loss=loss_fn(t_p, t_c)\n",
    "        loss.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            params -= params.grad*le\n",
    "            \n",
    "        if epoch%1000==0:\n",
    "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
    "        \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6a5d941e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        if params.grad is not None:  # <1>\n",
    "            params.grad.zero_()\n",
    "        \n",
    "        t_p = model(t_u, *params) \n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        loss.backward()\n",
    "        \n",
    "        with torch.no_grad():  # <2>\n",
    "            params -= learning_rate * params.grad\n",
    "\n",
    "        if epoch % 500 == 0:\n",
    "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
    "            \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d13e4fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_un=t_u*0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d2e1357a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 80.364342\n",
      "Epoch 1000, Loss 3.825483\n",
      "Epoch 2000, Loss 2.957596\n",
      "Epoch 3000, Loss 2.928646\n",
      "Epoch 4000, Loss 2.927679\n",
      "Epoch 5000, Loss 2.927648\n",
      "Epoch 6000, Loss 2.927647\n",
      "Epoch 7000, Loss 2.927645\n",
      "Epoch 8000, Loss 2.927645\n",
      "Epoch 9000, Loss 2.927645\n",
      "Epoch 10000, Loss 2.927645\n",
      "Epoch 11000, Loss 2.927645\n",
      "Epoch 12000, Loss 2.927645\n",
      "Epoch 13000, Loss 2.927645\n",
      "Epoch 14000, Loss 2.927645\n",
      "Epoch 15000, Loss 2.927645\n",
      "Epoch 16000, Loss 2.927645\n",
      "Epoch 17000, Loss 2.927645\n",
      "Epoch 18000, Loss 2.927645\n",
      "Epoch 19000, Loss 2.927645\n",
      "Epoch 20000, Loss 2.927645\n",
      "Epoch 21000, Loss 2.927645\n",
      "Epoch 22000, Loss 2.927645\n",
      "Epoch 23000, Loss 2.927645\n",
      "Epoch 24000, Loss 2.927645\n",
      "Epoch 25000, Loss 2.927645\n",
      "Epoch 26000, Loss 2.927645\n",
      "Epoch 27000, Loss 2.927645\n",
      "Epoch 28000, Loss 2.927645\n",
      "Epoch 29000, Loss 2.927645\n",
      "Epoch 30000, Loss 2.927645\n",
      "Epoch 31000, Loss 2.927645\n",
      "Epoch 32000, Loss 2.927645\n",
      "Epoch 33000, Loss 2.927645\n",
      "Epoch 34000, Loss 2.927645\n",
      "Epoch 35000, Loss 2.927645\n",
      "Epoch 36000, Loss 2.927645\n",
      "Epoch 37000, Loss 2.927645\n",
      "Epoch 38000, Loss 2.927645\n",
      "Epoch 39000, Loss 2.927645\n",
      "Epoch 40000, Loss 2.927645\n",
      "Epoch 41000, Loss 2.927645\n",
      "Epoch 42000, Loss 2.927645\n",
      "Epoch 43000, Loss 2.927645\n",
      "Epoch 44000, Loss 2.927645\n",
      "Epoch 45000, Loss 2.927645\n",
      "Epoch 46000, Loss 2.927645\n",
      "Epoch 47000, Loss 2.927645\n",
      "Epoch 48000, Loss 2.927645\n",
      "Epoch 49000, Loss 2.927645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3676, -17.3042], requires_grad=True)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(50000, 0.01, torch.tensor([1.0,0.0], requires_grad=True), t_un, t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "796eb4c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASGD',\n",
       " 'Adadelta',\n",
       " 'Adagrad',\n",
       " 'Adam',\n",
       " 'AdamW',\n",
       " 'Adamax',\n",
       " 'LBFGS',\n",
       " 'NAdam',\n",
       " 'Optimizer',\n",
       " 'RAdam',\n",
       " 'RMSprop',\n",
       " 'Rprop',\n",
       " 'SGD',\n",
       " 'SparseAdam',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_functional',\n",
       " '_multi_tensor',\n",
       " 'lr_scheduler',\n",
       " 'swa_utils']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "dir(optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "07e95117",
   "metadata": {},
   "outputs": [],
   "source": [
    "le=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6d381838",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=optim.SGD([params], lr=le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6c4e13c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "params=torch.tensor([1.0,0.0], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "670e3edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_p=model(t_u, *params)\n",
    "loss=loss_fn(t_p, t_c)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "63040781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.5173, -0.0826], requires_grad=True)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5e020609",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ccfa77a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14.0256,  0.2204], requires_grad=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "49dff4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.5483, -0.0083], requires_grad=True) tensor(1763.8848, grad_fn=<MeanBackward0>)\n",
      "tensor([ 0.2811, -2.7914], requires_grad=True) tensor(21.3763, grad_fn=<MeanBackward0>)\n",
      "tensor([ 0.3222, -5.1227], requires_grad=True) tensor(15.9254, grad_fn=<MeanBackward0>)\n",
      "tensor([ 0.3567, -7.0796], requires_grad=True) tensor(12.0850, grad_fn=<MeanBackward0>)\n",
      "tensor([ 0.3856, -8.7221], requires_grad=True) tensor(9.3793, grad_fn=<MeanBackward0>)\n",
      "tensor([  0.4099, -10.1008], requires_grad=True) tensor(7.4731, grad_fn=<MeanBackward0>)\n",
      "tensor([  0.4303, -11.2580], requires_grad=True) tensor(6.1301, grad_fn=<MeanBackward0>)\n",
      "tensor([  0.4474, -12.2293], requires_grad=True) tensor(5.1839, grad_fn=<MeanBackward0>)\n",
      "tensor([  0.4617, -13.0446], requires_grad=True) tensor(4.5172, grad_fn=<MeanBackward0>)\n",
      "tensor([  0.4738, -13.7289], requires_grad=True) tensor(4.0476, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for x in range(100000):\n",
    "    optimizer.zero_grad()\n",
    "    t_p=model(t_u, *params)\n",
    "    loss=loss_fn(t_p, t_c)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if x%10000==0:\n",
    "        print(params, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3ac22bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(epochs, optimizer, params, tu, tc):\n",
    "    for epoch in range(epochs):\n",
    "        tp=model(tu, *params)\n",
    "        loss=loss_fn(tp,tc)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #if epoch % 10000==0:\n",
    "            #print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
    "            \n",
    "    return params, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4194c4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "params=torch.tensor([1.0,0.0], requires_grad=True)\n",
    "opt=optim.Adam([params], 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "849898d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 1763.884766\n",
      "Epoch 1000, Loss 1346.007935\n",
      "Epoch 2000, Loss 1003.108826\n",
      "Epoch 3000, Loss 723.590576\n",
      "Epoch 4000, Loss 499.759247\n",
      "Epoch 5000, Loss 326.218567\n",
      "Epoch 6000, Loss 198.478989\n",
      "Epoch 7000, Loss 111.823143\n",
      "Epoch 8000, Loss 60.302658\n",
      "Epoch 9000, Loss 35.834106\n",
      "Epoch 10000, Loss 28.045321\n",
      "Epoch 11000, Loss 26.782225\n",
      "Epoch 12000, Loss 26.596064\n",
      "Epoch 13000, Loss 26.406122\n",
      "Epoch 14000, Loss 26.171558\n",
      "Epoch 15000, Loss 25.910223\n",
      "Epoch 16000, Loss 25.637245\n",
      "Epoch 17000, Loss 25.360655\n",
      "Epoch 18000, Loss 25.083834\n",
      "Epoch 19000, Loss 24.808100\n",
      "Epoch 20000, Loss 24.533850\n",
      "Epoch 21000, Loss 24.261189\n",
      "Epoch 22000, Loss 23.990257\n",
      "Epoch 23000, Loss 23.721056\n",
      "Epoch 24000, Loss 23.453590\n",
      "Epoch 25000, Loss 23.187853\n",
      "Epoch 26000, Loss 22.923851\n",
      "Epoch 27000, Loss 22.661573\n",
      "Epoch 28000, Loss 22.401033\n",
      "Epoch 29000, Loss 22.142225\n",
      "Epoch 30000, Loss 21.885149\n",
      "Epoch 31000, Loss 21.629801\n",
      "Epoch 32000, Loss 21.376181\n",
      "Epoch 33000, Loss 21.124298\n",
      "Epoch 34000, Loss 20.874144\n",
      "Epoch 35000, Loss 20.625721\n",
      "Epoch 36000, Loss 20.379030\n",
      "Epoch 37000, Loss 20.134277\n",
      "Epoch 38000, Loss 19.891630\n",
      "Epoch 39000, Loss 19.650703\n",
      "Epoch 40000, Loss 19.411501\n",
      "Epoch 41000, Loss 19.174019\n",
      "Epoch 42000, Loss 18.938265\n",
      "Epoch 43000, Loss 18.704233\n",
      "Epoch 44000, Loss 18.471926\n",
      "Epoch 45000, Loss 18.241335\n",
      "Epoch 46000, Loss 18.012468\n",
      "Epoch 47000, Loss 17.785332\n",
      "Epoch 48000, Loss 17.559917\n",
      "Epoch 49000, Loss 17.336220\n",
      "Epoch 50000, Loss 17.114250\n",
      "Epoch 51000, Loss 16.894001\n",
      "Epoch 52000, Loss 16.675478\n",
      "Epoch 53000, Loss 16.458672\n",
      "Epoch 54000, Loss 16.243591\n",
      "Epoch 55000, Loss 16.030237\n",
      "Epoch 56000, Loss 15.818604\n",
      "Epoch 57000, Loss 15.608694\n",
      "Epoch 58000, Loss 15.400512\n",
      "Epoch 59000, Loss 15.194047\n",
      "Epoch 60000, Loss 14.989305\n",
      "Epoch 61000, Loss 14.786292\n",
      "Epoch 62000, Loss 14.584992\n",
      "Epoch 63000, Loss 14.385422\n",
      "Epoch 64000, Loss 14.187572\n",
      "Epoch 65000, Loss 13.991451\n",
      "Epoch 66000, Loss 13.797050\n",
      "Epoch 67000, Loss 13.604369\n",
      "Epoch 68000, Loss 13.413418\n",
      "Epoch 69000, Loss 13.224182\n",
      "Epoch 70000, Loss 13.036670\n",
      "Epoch 71000, Loss 12.850890\n",
      "Epoch 72000, Loss 12.666823\n",
      "Epoch 73000, Loss 12.484482\n",
      "Epoch 74000, Loss 12.303864\n",
      "Epoch 75000, Loss 12.124972\n",
      "Epoch 76000, Loss 11.947799\n",
      "Epoch 77000, Loss 11.772355\n",
      "Epoch 78000, Loss 11.598626\n",
      "Epoch 79000, Loss 11.426624\n",
      "Epoch 80000, Loss 11.256347\n",
      "Epoch 81000, Loss 11.087796\n",
      "Epoch 82000, Loss 10.920960\n",
      "Epoch 83000, Loss 10.755850\n",
      "Epoch 84000, Loss 10.592464\n",
      "Epoch 85000, Loss 10.430800\n",
      "Epoch 86000, Loss 10.270862\n",
      "Epoch 87000, Loss 10.112648\n",
      "Epoch 88000, Loss 9.956152\n",
      "Epoch 89000, Loss 9.801379\n",
      "Epoch 90000, Loss 9.648333\n",
      "Epoch 91000, Loss 9.497006\n",
      "Epoch 92000, Loss 9.347403\n",
      "Epoch 93000, Loss 9.199531\n",
      "Epoch 94000, Loss 9.053371\n",
      "Epoch 95000, Loss 8.908937\n",
      "Epoch 96000, Loss 8.766231\n",
      "Epoch 97000, Loss 8.625243\n",
      "Epoch 98000, Loss 8.485984\n",
      "Epoch 99000, Loss 8.348439\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3999, -9.5367], requires_grad=True)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tu=t_u\n",
    "tc=t_c\n",
    "training_loop(100000, opt, params, tu, tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5ebe31da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optim.RAdam([params], 0.01)\n",
      "--------------RAdam--------------\n",
      "(tensor([  5.3677, -17.3048], requires_grad=True), tensor(2.9276, grad_fn=<MeanBackward0>))\n",
      "optim.RMSprop([params], 0.01)\n",
      "--------------RMSprop--------------\n",
      "(tensor([  5.3627, -17.3098], requires_grad=True), tensor(2.9287, grad_fn=<MeanBackward0>))\n",
      "optim.Rprop([params], 0.01)\n",
      "--------------Rprop--------------\n",
      "(tensor([  5.3677, -17.3047], requires_grad=True), tensor(2.9276, grad_fn=<MeanBackward0>))\n",
      "optim.SGD([params], 0.01)\n",
      "--------------SGD--------------\n",
      "(tensor([  5.3676, -17.3042], requires_grad=True), tensor(2.9276, grad_fn=<MeanBackward0>))\n",
      "optim.SparseAdam([params], 0.01)\n",
      "--------------SparseAdam--------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "SparseAdam does not support dense gradients, please consider Adam instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/nf/8m8xlbhs4yg7rbtnlz7wymtr0000gn/T/ipykernel_4959/159705375.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt_c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--------------\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"--------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/nf/8m8xlbhs4yg7rbtnlz7wymtr0000gn/T/ipykernel_4959/1225607969.py\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(epochs, optimizer, params, tu, tc)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m#if epoch % 10000==0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.9/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.9/site-packages/torch/optim/sparse_adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     78\u001b[0m                     \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SparseAdam does not support dense gradients, please consider Adam instead'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                     \u001b[0mgrads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: SparseAdam does not support dense gradients, please consider Adam instead"
     ]
    }
   ],
   "source": [
    "for opt in dir(optim)[9:]:\n",
    "    params=torch.tensor([1.0,0.0], requires_grad=True)\n",
    "    print(\"optim.\"+str(opt)+\"([params], 0.01)\")\n",
    "    optimizer=eval(\"optim.\"+str(opt)+\"([params], 0.01)\")\n",
    "    tu=t_u*0.1\n",
    "    tc=t_c\n",
    "    print(\"--------------\"+str(opt)+\"--------------\")\n",
    "    print(training_loop(100000, optimizer, params, tu, tc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f0f3d120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3,  1,  6,  9,  0,  5,  2, 10,  8,  4,  7])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = t_u.shape[0]\n",
    "n_val = int(0.2 * n_samples)\n",
    "\n",
    "shuffled_indices = torch.randperm(n_samples)\n",
    "shuffled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5c7f49f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices=shuffled_indices[:-n_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "62d56085",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_indices=shuffled_indices[-n_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e28da9f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3,  1,  6,  9,  0,  5,  2, 10,  8]), tensor([4, 7]))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_indices, val_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "22f727db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tu=tu[train_indices]\n",
    "train_tc=tc[train_indices]\n",
    "\n",
    "val_tu=tu[val_indices]\n",
    "val_tc=tc[val_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f5dc0916",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tu*=0.1\n",
    "train_tu*=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "60d9a588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(epochs, optimizer, params, train_tu, val_tu, train_tc, val_tc):\n",
    "    for epoch in range(epochs):\n",
    "        train_tp=model(train_tu, *params)\n",
    "        train_loss=loss_fn(train_tp, train_tc)\n",
    "        \n",
    "        val_tp=model(val_tu, *params)\n",
    "        val_loss=loss_fn(val_tp, val_tc)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 1000==0:\n",
    "            print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
    "                  f\" Validation loss {val_loss.item():.4f}\")\n",
    "            \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9420dfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Training loss 198.6322, Validation loss 63.3613\n",
      "Epoch 1000, Training loss 2.7935, Validation loss 3.6385\n",
      "Epoch 2000, Training loss 2.6958, Validation loss 5.2070\n",
      "Epoch 3000, Training loss 2.6956, Validation loss 5.2858\n",
      "Epoch 4000, Training loss 2.6956, Validation loss 5.2889\n",
      "Epoch 5000, Training loss 2.6956, Validation loss 5.2889\n",
      "Epoch 6000, Training loss 2.6956, Validation loss 5.2889\n",
      "Epoch 7000, Training loss 2.6956, Validation loss 5.2889\n",
      "Epoch 8000, Training loss 2.6956, Validation loss 5.2889\n",
      "Epoch 9000, Training loss 2.6956, Validation loss 5.2889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 56.7098, -18.9269], requires_grad=True)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params=torch.tensor([1.0,0.0], requires_grad=True)\n",
    "le=0.1\n",
    "opt=optim.SGD([params], le)\n",
    "\n",
    "training_loop(10000, opt, params, train_tu, val_tu, train_tc, val_tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe44c0da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
